{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Augmentation"
      ],
      "metadata": {
        "id": "yj2sh8DgHMd7"
      },
      "id": "yj2sh8DgHMd7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WP2hiwVGoWA"
      },
      "outputs": [],
      "source": [
        "# reading data\n",
        "import pandas as pd\n",
        "import os\n",
        "from nlpaug.util.file.download import DownloadUtil\n",
        "df = pd.read_csv(\"new_data.csv\")\n",
        "df"
      ],
      "id": "-WP2hiwVGoWA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0TwFb82HbOu"
      },
      "outputs": [],
      "source": [
        "# library for augmentation\n",
        "!pip install nlpaug"
      ],
      "id": "j0TwFb82HbOu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "750CNucgSdXb"
      },
      "outputs": [],
      "source": [
        "# installing transformers\n",
        "!pip install transformers"
      ],
      "id": "750CNucgSdXb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2NBD03i1QDz"
      },
      "outputs": [],
      "source": [
        "# installing specific version to avoid error\n",
        "!pip install gensim==4.2"
      ],
      "id": "J2NBD03i1QDz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRYze54VH3tk"
      },
      "outputs": [],
      "source": [
        "# downloading the models\n",
        "DownloadUtil.download_word2vec(dest_dir='.') # word2vec model\n",
        "DownloadUtil.download_glove(model_name='glove.6B', dest_dir='.') # GloVe model\n",
        "DownloadUtil.download_fasttext(model_name='wiki-news-300d-1M', dest_dir='.') # fasttext model"
      ],
      "id": "fRYze54VH3tk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SohAbTraGoWD"
      },
      "outputs": [],
      "source": [
        "# importing for augmentation\n",
        "import nlpaug.augmenter.word as naw\n",
        "import nlpaug.augmenter.sentence as nas"
      ],
      "id": "SohAbTraGoWD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLVSQpDX_XiJ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "FLVSQpDX_XiJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3B_tlCLGoWD"
      },
      "outputs": [],
      "source": [
        "# dictionary of models for augmentation\n",
        "model_type_to_path_map = {\n",
        "    \"word2vec\" : \"GoogleNews-vectors-negative300.bin\",\n",
        "    \"glove\" : \"glove.6B.300d.txt\",\n",
        "    \"fasttext\" : \"wiki-news-300d-1M.vec\"\n",
        "}\n",
        "\n",
        "# empty data holder\n",
        "data_holder_list = []\n",
        "\n",
        "# inclusion of augmented data in corresponding row of the dataframe\n",
        "def include_in_row(row, augmented_texts, augmenter_name):\n",
        "\n",
        "  # for each different augmented text\n",
        "  for i, at in enumerate(augmented_texts):\n",
        "\n",
        "    # updating row\n",
        "      row[augmenter_name + str(i)] = at\n",
        "\n",
        "  return row\n",
        "\n",
        "# each augmenter will produce 2 different augmentation for each data point (n = 2)\n",
        "for index, row in df.iterrows():\n",
        "    print(f\"Augmenting entry: {index}\")\n",
        "    text = row['message']\n",
        "\n",
        "    #Substitute word by spelling mistake words dictionary\n",
        "    augmenter_name = \"SpellingAug\"\n",
        "    aug = naw.SpellingAug()\n",
        "    augmented_texts = aug.augment(text, n=2)\n",
        "    row = include_in_row(row, augmented_texts, augmenter_name)\n",
        "\n",
        "    # Insert word randomly by word embeddings similarity\n",
        "    # model_type: word2vec, glove or fasttext\n",
        "    for key, value in model_type_to_path_map.items():\n",
        "      # insert augment\n",
        "      augmenter_name = key + \" \" + value + \" \" + \"insert\"\n",
        "      aug = naw.WordEmbsAug(\n",
        "          model_type = key,\n",
        "          model_path = value,\n",
        "          action = \"insert\")\n",
        "      augmented_texts = aug.augment(text, n=2)\n",
        "      row = include_in_row(row, augmented_texts, augmenter_name)\n",
        "\n",
        "      # substitute augment\n",
        "      augmenter_name = key + \" \" + value + \" \" + \"substitute\"\n",
        "      aug = naw.WordEmbsAug(\n",
        "          model_type = key,\n",
        "          model_path = value,\n",
        "          action = \"substitute\")\n",
        "      augmented_texts = aug.augment(text, n=2)\n",
        "      row = include_in_row(row, augmented_texts, augmenter_name)\n",
        "\n",
        "    # contextual insert augment using bert-base-uncased\n",
        "    augmenter_name = \"ContextualWordEmbsAug bert-base-uncased insert\"\n",
        "    aug = naw.ContextualWordEmbsAug(\n",
        "    model_path='bert-base-uncased', action=\"insert\")\n",
        "    augmented_texts = aug.augment(text, n=2)\n",
        "    row = include_in_row(row, augmented_texts, augmenter_name)\n",
        "\n",
        "    # contextual substitute augment using bert-base-uncased\n",
        "    augmenter_name = \"ContextualWordEmbsAug bert-base-uncased substitute\"\n",
        "    aug = naw.ContextualWordEmbsAug(\n",
        "    model_path='bert-base-uncased', action=\"substitute\")\n",
        "    augmented_texts = aug.augment(text, n=2)\n",
        "    row = include_in_row(row, augmented_texts, augmenter_name)\n",
        "\n",
        "    # contextual substitute augment using distilbert-base-uncased\n",
        "    augmenter_name = \"ContextualWordEmbsAug distilbert-base-uncased substitute\"\n",
        "    aug = naw.ContextualWordEmbsAug(\n",
        "    model_path='distilbert-base-uncased', action=\"substitute\")\n",
        "    augmented_texts = aug.augment(text, n=2)\n",
        "    row = include_in_row(row, augmented_texts, augmenter_name)\n",
        "\n",
        "    # # contextual substitute augment roberta-base\n",
        "    augmenter_name = \"ContextualWordEmbsAug roberta-base substitute\"\n",
        "    aug = naw.ContextualWordEmbsAug(\n",
        "    model_path='roberta-base', action=\"substitute\")\n",
        "    augmented_texts = aug.augment(text, n=2)\n",
        "    row = include_in_row(row, augmented_texts, augmenter_name)\n",
        "\n",
        "    # synonym augmentation\n",
        "    augmenter_name = \"SynonymAug wordnet\"\n",
        "    aug = naw.SynonymAug(aug_src='wordnet')\n",
        "    augmented_texts = aug.augment(text, n=2)\n",
        "    row = include_in_row(row, augmented_texts, augmenter_name)\n",
        "\n",
        "    # saving directly to google drive for later use\n",
        "    row.to_csv(f\"/content/drive/MyDrive/aug/{index}.csv\")"
      ],
      "id": "m3B_tlCLGoWD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xANp30-rf7Ei"
      },
      "outputs": [],
      "source": [
        "# empty dataframe\n",
        "df_m = pd.DataFrame()\n",
        "\n",
        "# rading all rows saved in drive and\n",
        "# creating a full dataframe\n",
        "for file_ in os.listdir(\"/content/drive/MyDrive/aug/\"):\n",
        "  df = pd.read_csv(\"/content/drive/MyDrive/aug/\" + file_)\n",
        "  df_m = pd.concat([df_m, df.T])"
      ],
      "id": "xANp30-rf7Ei"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAynsJnbglug"
      },
      "outputs": [],
      "source": [
        "# saving the updated dataframe with augmented text\n",
        "df_m.drop_duplicates().to_csv(\"new_aug.csv\")"
      ],
      "id": "CAynsJnbglug"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "pPUvoTO9HWbl"
      },
      "id": "pPUvoTO9HWbl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTUAm4Xxf5xa"
      },
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from bs4 import BeautifulSoup\n",
        "import plotly.graph_objs as go\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "import plotly.figure_factory as ff\n",
        "InteractiveShell.ast_node_interactivity = 'all'\n",
        "from plotly.offline import iplot"
      ],
      "id": "XTUAm4Xxf5xa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-CI5qBMcJq8"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, SpatialDropout1D\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "id": "A-CI5qBMcJq8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tk3BEAS5f5u6"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"updated_final_df.csv\")\n",
        "df['message'] = df['message'].astype(str)\n",
        "df"
      ],
      "id": "Tk3BEAS5f5u6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Pre-processing\n"
      ],
      "metadata": {
        "id": "QNqkdC-fQgwo"
      },
      "id": "QNqkdC-fQgwo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3ORMcN4gLLt"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    text: a string\n",
        "\n",
        "    return: cleaned text\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    text = replace_symbols.sub(' ', text)\n",
        "    text = bad_symbols.sub('', text)\n",
        "    text = ' '.join(word for word in text.split() if word not in stopwords)\n",
        "    return text\n",
        "\n",
        "# replace symbols by space in text\n",
        "replace_symbols = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "\n",
        "# remove symbols wfrom text\n",
        "bad_symbols = re.compile('[^0-9a-z #+_]')\n",
        "\n",
        "# remove stopwors from text\n",
        "stopwords = set(stopwords.words('english'))\n",
        "\n",
        "# cleaning the data and adding to the same column\n",
        "df['message'] = df['message'].apply(clean_text)\n",
        "df"
      ],
      "id": "a3ORMcN4gLLt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPaN9O2tgMwt"
      },
      "outputs": [],
      "source": [
        "# The maximum number of words to be used\n",
        "max_words = 50000\n",
        "\n",
        "# Max number of words in each complaint.\n",
        "max_seq_len = 100\n",
        "\n",
        "# This is fixed.\n",
        "embeddign_dim = 100\n",
        "\n",
        "# tokenizing texts\n",
        "tokenizer = Tokenizer(num_words = max_words, filters = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower = True)\n",
        "tokenizer.fit_on_texts(df['message'].values)\n",
        "word_index = tokenizer.word_index\n",
        "print(f'Unique tokens: {len(word_index)}')"
      ],
      "id": "mPaN9O2tgMwt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G83bL3fzgOLU"
      },
      "outputs": [],
      "source": [
        "# defining X\n",
        "X = tokenizer.texts_to_sequences(df['message'].values)\n",
        "X = pad_sequences(X, maxlen = max_seq_len)\n",
        "print(f'Shape X: {X.shape}')"
      ],
      "id": "G83bL3fzgOLU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1pN0Z6LgPVZ"
      },
      "outputs": [],
      "source": [
        "# getting Y\n",
        "Y = pd.get_dummies(df['topic_field']).values\n",
        "print('Shape Y: {Y.shape}')"
      ],
      "id": "r1pN0Z6LgPVZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08Ix5eCv3gMu"
      },
      "outputs": [],
      "source": [
        "# newly added data were from room 0\n",
        "newly_added_df = df[df['room_number'] == 0]\n",
        "display(newly_added_df)\n",
        "\n",
        "# getting the original data to df DataFrame\n",
        "df = df[df['room_number'] != 0]\n",
        "display(df)"
      ],
      "id": "08Ix5eCv3gMu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TT_23FuggQtP"
      },
      "outputs": [],
      "source": [
        "# also, reasing the base dataframe to include the corresponding augmented\n",
        "# text only to train set\n",
        "\n",
        "# It is necessary, as if we include a augmented version of original text\n",
        "# in training, and if somehow another version is on test, then\n",
        "# it will bias the model evaluation\n",
        "\n",
        "# to fix this issue, it is necessary\n",
        "df_ = pd.read_csv(\"base_df.csv\")\n",
        "df_x = df_[\"message\"]\n",
        "df_y = df_[\"field\"]\n",
        "\n",
        "# train test split on the base data\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(df_x, df_y, test_size = 0.10, random_state = 42)"
      ],
      "id": "TT_23FuggQtP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlIWd3KutEz4"
      },
      "outputs": [],
      "source": [
        "# getting value counts of training texts and corresponding classes\n",
        "Y_train.value_counts()"
      ],
      "id": "KlIWd3KutEz4"
    },
    {
      "cell_type": "code",
      "source": [
        "# getting value counts of test texts and corresponding classes\n",
        "Y_test.value_counts()"
      ],
      "metadata": {
        "id": "GcNGAlD_n0nE"
      },
      "id": "GcNGAlD_n0nE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJYkW_RnkncH"
      },
      "outputs": [],
      "source": [
        "# Our X is containing all data including augmented\n",
        "# and after tokenization\n",
        "\n",
        "# we will use this function to get the newly splitted original\n",
        "# texts and their augmented version to be in either\n",
        "# training or test split\n",
        "\n",
        "def get_tranformed(transformed, original_df):\n",
        "  \"\"\"\n",
        "  Gets all tokenized sequences as transformed, and splitted dataframe as original_df\n",
        "\n",
        "  outputs the augmented versions and original text to be in the specific set as return_arr\n",
        "  \"\"\"\n",
        "  # placeholder for data\n",
        "  return_arr = []\n",
        "\n",
        "  for index, row in original_df.items():\n",
        "    # getting the original index\n",
        "    original_index = index\n",
        "\n",
        "    # getting all data that have the original index\n",
        "    temp_ = df[df['original_df_index'] == original_index]\n",
        "\n",
        "    # adding each row to the placeholder to return\n",
        "    for idx, row in temp_.iterrows():\n",
        "      return_arr.append(transformed[idx])\n",
        "\n",
        "  return return_arr"
      ],
      "id": "HJYkW_RnkncH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_l0z3IgkwpQ"
      },
      "outputs": [],
      "source": [
        "# getting all augmented version of the data into same split\n",
        "X_train_n = get_tranformed(X, X_train)\n",
        "X_test_n = get_tranformed(X, X_test)\n",
        "Y_train_n = get_tranformed(Y, Y_train)\n",
        "Y_test_n = get_tranformed(Y, Y_test)\n",
        "\n",
        "print(f\"# of Train data: {len(X_train_n)}\")\n",
        "print(f\"# of Test data: {len(X_test_n)}\")"
      ],
      "id": "x_l0z3IgkwpQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjprQBjh1mZZ"
      },
      "outputs": [],
      "source": [
        "# Now, adding the newly added data to the train set, only\n",
        "for index, row in newly_added_df.iterrows():\n",
        "  X_train_n.append(X[index])\n",
        "  Y_train_n.append(Y[index])\n",
        "\n",
        "# transforming to numpy array\n",
        "X_train_n = np.array(X_train_n)\n",
        "X_test_n = np.array(X_test_n)\n",
        "Y_train_n = np.array(Y_train_n)\n",
        "Y_test_n = np.array(Y_test_n)\n",
        "\n",
        "print(f\"Train data shape: {X_train_n.shape}\")\n",
        "print(f\"Test data shape: {X_test_n.shape}\")"
      ],
      "id": "WjprQBjh1mZZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SZZy67V4br2"
      },
      "outputs": [],
      "source": [
        "# checking if both set got the actual amount of data\n",
        "assert (X_train_n.shape[0] + X_test_n.shape[0]) == df.shape[0]"
      ],
      "id": "7SZZy67V4br2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple LSTM Model"
      ],
      "metadata": {
        "id": "x3U92aQwQqnB"
      },
      "id": "x3U92aQwQqnB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9X7xvQ02gR8r"
      },
      "outputs": [],
      "source": [
        "# defining model\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embeddign_dim, input_length = X.shape[1]))\n",
        "model.add(SpatialDropout1D(0.3))\n",
        "model.add(LSTM(64, dropout = 0.3, recurrent_dropout = 0.2)) # .2, .3, .4, .5\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "print(model.summary())"
      ],
      "id": "9X7xvQ02gR8r"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZprLPQmEgTwr"
      },
      "outputs": [],
      "source": [
        "# train hyperparameters\n",
        "epochs = 3\n",
        "batch_size = 32\n",
        "\n",
        "history = model.fit(X_train_n, Y_train_n,\n",
        "                    epochs = epochs,\n",
        "                    batch_size = batch_size,\n",
        "                    validation_split = 0.2,\n",
        "                    callbacks = [EarlyStopping(monitor = 'val_loss', patience = 3, min_delta = 0.0001)])"
      ],
      "id": "ZprLPQmEgTwr"
    },
    {
      "cell_type": "code",
      "source": [
        "accr = model.evaluate(X_test_n, Y_test_n)\n",
        "print(f'Test set >  Loss: {accr[0] : 0.3f},  Accuracy: {accr[1] : 0.3f}')"
      ],
      "metadata": {
        "id": "ItxAKsqxyg4d"
      },
      "id": "ItxAKsqxyg4d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to check generated data\n",
        "def test_model(model, df_name):\n",
        "    \"\"\"\n",
        "    this function will predict anything any message text availabe on df_name given a model\n",
        "\n",
        "    will return a dictionary dictionary with actual label and predicted label\n",
        "    \"\"\"\n",
        "    # initial correct and wrong\n",
        "    correct = 0\n",
        "    wrong = 0\n",
        "\n",
        "    result = {}\n",
        "\n",
        "    # for each data point\n",
        "    for index, row in pd.read_csv(df_name).iterrows():\n",
        "        new_complaint = [row['message']]\n",
        "\n",
        "        # transforming to get prediction\n",
        "        seq = tokenizer.texts_to_sequences(new_complaint)\n",
        "        padded = pad_sequences(seq, maxlen = max_words)\n",
        "\n",
        "        # getting prediction\n",
        "        pred = model.predict(padded)\n",
        "        labels = df.topic_field.unique().tolist()\n",
        "        predict_label = labels[np.argmax(pred)]\n",
        "\n",
        "        # adding actual labels and predicted labels\n",
        "        result[index] = {\n",
        "            \"actual_label\" : row['category'],\n",
        "            \"predicted_label\" : predict_label\n",
        "        }\n",
        "\n",
        "        # correct if actual label is same as the predicted label\n",
        "        if predict_label == row['category']:\n",
        "            correct += 1\n",
        "        else:\n",
        "            wrong += 1\n",
        "\n",
        "    print(f\"Right: {correct}\\tWrong: {wrong}\")\n",
        "\n",
        "    return result\n",
        "\n",
        "# testing the model on holdout data\n",
        "test_model(model, \"new_data.csv\")"
      ],
      "metadata": {
        "id": "femk6OHiyzH2"
      },
      "id": "femk6OHiyzH2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bidirectional LSTM model"
      ],
      "metadata": {
        "id": "rynmlQmgQwwQ"
      },
      "id": "rynmlQmgQwwQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jd_CDzYacnz2"
      },
      "outputs": [],
      "source": [
        "# model architecture\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim = len(tokenizer.word_index) + 1, output_dim = 100, input_length = max_words))\n",
        "model.add(Bidirectional(LSTM(64, dropout = 0.2, recurrent_dropout = 0.2, return_sequences = True)))\n",
        "model.add(Bidirectional(LSTM(32, dropout = 0.2, recurrent_dropout = 0.2)))\n",
        "model.add(Dense(64, activation = 'relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(10, activation = 'softmax'))\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "print(model.summary())"
      ],
      "id": "Jd_CDzYacnz2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxCLvcW4c-zQ"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "epochs = 3\n",
        "batch_size = 32\n",
        "\n",
        "history = model.fit(X_train_n, Y_train_n,\n",
        "                    epochs = epochs,\n",
        "                    batch_size = batch_size,\n",
        "                    validation_split = 0.2,\n",
        "                    callbacks = [EarlyStopping(monitor = 'val_loss', patience = 3, min_delta = 0.0001)])"
      ],
      "id": "zxCLvcW4c-zQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewdUxgOJgVwu"
      },
      "outputs": [],
      "source": [
        "# model performance\n",
        "accr = model.evaluate(X_test_n,Y_test_n)\n",
        "print(f'Test set >  Loss: {accr[0] : 0.3f},  Accuracy: {accr[1] : 0.3f}')"
      ],
      "id": "ewdUxgOJgVwu"
    },
    {
      "cell_type": "code",
      "source": [
        "# testing the model on the holdout data\n",
        "test_model(model, \"new_data.csv\")"
      ],
      "metadata": {
        "id": "8uqVYnEonFxN"
      },
      "id": "8uqVYnEonFxN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wordcloud visualization"
      ],
      "metadata": {
        "id": "WbcHnq9-RV6w"
      },
      "id": "WbcHnq9-RV6w"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tueplots"
      ],
      "metadata": {
        "id": "XGEoLqEHsniQ"
      },
      "id": "XGEoLqEHsniQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from tueplots import bundles\n",
        "plt.rcParams.update(bundles.icml2022())\n",
        "import tueplots.constants.color.palettes as tue_palettes\n",
        "\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams.update(mpl.rcParamsDefault)"
      ],
      "metadata": {
        "id": "IOrPljuUoAvJ"
      },
      "id": "IOrPljuUoAvJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# augmented dataframe\n",
        "df"
      ],
      "metadata": {
        "id": "SsnpiJ18stkJ"
      },
      "id": "SsnpiJ18stkJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get raw train data after split\n",
        "train_raw = df[df['original_df_index'].apply(lambda x: x in X_train.index)]\n",
        "display(train_raw)\n",
        "\n",
        "# get raw test data after split\n",
        "test_raw = df[df['original_df_index'].apply(lambda x: x in X_test.index)]\n",
        "display(test_raw)"
      ],
      "metadata": {
        "id": "fcC3Zpp8t-xS"
      },
      "id": "fcC3Zpp8t-xS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a long string out of texts, on the whole dataset\n",
        "long_string = ','.join(list(df['message'].values))\n",
        "\n",
        "# wordcloud object\n",
        "wordcloud = WordCloud(background_color = \"white\", max_words = 5000, contour_width = 3, contour_color = 'steelblue')\n",
        "\n",
        "# generating a word cloud\n",
        "wordcloud.generate(long_string)\n",
        "\n",
        "# visualizing it\n",
        "wordcloud.to_image()"
      ],
      "metadata": {
        "id": "JyKIaWDmuft6"
      },
      "id": "JyKIaWDmuft6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# on training dataset\n",
        "long_string = ','.join(list(train_raw['message'].values))\n",
        "\n",
        "# wordcloud object\n",
        "wordcloud = WordCloud(background_color = \"white\", max_words = 5000, contour_width = 3, contour_color = 'steelblue')\n",
        "\n",
        "# generating a word cloud\n",
        "wordcloud.generate(long_string)\n",
        "\n",
        "# visualizing it\n",
        "wordcloud.to_image()"
      ],
      "metadata": {
        "id": "naajR8Apu8zy"
      },
      "id": "naajR8Apu8zy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# on test dataset\n",
        "long_string = ','.join(list(test_raw['message'].values))\n",
        "\n",
        "# wordcloud object\n",
        "wordcloud = WordCloud(background_color = \"white\", max_words = 5000, contour_width = 3, contour_color = 'steelblue')\n",
        "\n",
        "# generating a word cloud\n",
        "wordcloud.generate(long_string)\n",
        "\n",
        "# visualizing it\n",
        "wordcloud.to_image()"
      ],
      "metadata": {
        "id": "ew7y1W7evMFk"
      },
      "id": "ew7y1W7evMFk",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}